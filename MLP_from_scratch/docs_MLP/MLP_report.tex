\documentclass[a4paper,10pt,twocolumn]{article}
\title{\textbf{\underline{REPORT: Neural Networks}}}
\usepackage[left=2cm, right=2cm, top=1.5cm]{geometry}
\usepackage{xcolor}
\usepackage{ragged2e}
\usepackage{lipsum,multicol}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{amsmath, amsfonts}
\usepackage[english]{babel}
\usepackage{times}
\date{}
\author{}
\begin{document}
\maketitle
%===================================================================================================================%
\section*{\textcolor{blue}{1. Introduction:}}
In this report, we try to analyse the performance (accuracy, fscore, recall and precision) of a neural network that we built using NumPy. The implementation of this network is similar to that of Keras and the whole code has been written in a completely modularized fashion.

\section*{\textcolor{blue}{2. Data Preprocessing:}}
The dataset $housepricesdata.csv$ contained 1460 data points, each containing 10 input features (representing house features) and one binary target variable (representing if the house price is above or below the median price). In the preprocessing stage, all features were min-max scaled and randomly split into \textit{train\_set} and \textit{test\_set} (80-20 split ratio).

\section*{\textcolor{blue}{3. Activation functions:}}
\begin{itemize}
\item{linear: $y = x$}
\item{tanh: $y$ = $\frac{e^x - e^{-x}}{e^x + e^{-x}}$}
\item{sigmoid: $y$ = $\frac{1}{1 + e^{-x}}$}
\item{ReLU: $y$ = $max(0, x)$}
\end{itemize}

\section*{\textcolor{blue}{4. Loss function:}}
\begin{itemize}
\item{$L_n = -t_nlog(y_n)-(1-t_n)log(1-y_n)$} 
\end{itemize}

\section*{\textcolor{blue}{5. Network Architecture:}}
\subsection*{\textcolor{red}{5.1 Two-Layer NN}}
The first model we built was a 2-layer neural network (i.e. with one hidden layer). We experimented with activations like linear, ReLU and tanh for the hidden layer. The input layer had 10 neuron units with a ReLU activation (for best results. A slight drop in accuracy was observed when linear activation was used for the input layer). The final layer had an activation of sigmoid for binary classification. Output value greater than or equal to 0.5 was taken to represent positive classes and less than 0.5 to represent negative classes.
\texttt{np.random.random()} and \texttt{np.random.randn()} methods were used to randomly initialize weights and biases associated network to initialize the weights according to a Uniform or Guassian distribution respectively. Best results were obtained using uniformly distributed weights/biases with a \emph{seed=5} and 4 hidden neurons with ReLU and \emph{learning\_rate=0.001} with 20000 training epochs. The loss was observed to have more fluctuations using this configuration after 40000 epochs and thus lowering the accuracy (images attached below). The avg. loss using this configuration was observed to be 0.7 (per data point). The highest accuracy achieved by this model using this configuration was 87.67\% and the lowest accuracy was 75\% with an average accuracy of 81\%.
\begin{figure}[h]
\centering
\includegraphics[scale=1.0, width=5cm]{Fig11.png}
\caption{Plot of Loss vs Epochs}
\includegraphics[scale=1.0, width=5cm]{Fig12.png}
\caption{Fluctuations appear from 40000 iterations}
\end{figure}
\newline
\underline{Top metrics achieved by the model:}
\begin{itemize}
\item{Best testing accuracy: $87.67\%$}
\item{Best testing recall: $97.33\%$}
\item{Best testing precision: $80.11\%$}
\item{Best testing fscore: $86.50\%$}
\end{itemize}

\subsection*{\textcolor{red}{5.2 Three-Layer NN}}
The second model we built and experimented with was a three layer neural network. The accuracy of the three layer neural network was found to be better than that of the two layer network. Both the hidden layers had 5 neurons with ReLU activation for optimal results. Training the network, in this case, required more iterations as there were more number of parameter to be tuned than the two layer network. The optimal number of neurons for both hidden layers were chosen accordingly as suggested in research article \textit{Review on Methods to Fix Number of Hidden Neurons in Neural Networks} by Gnana Sheela et al. Once again, best results were obtained using uniform distribution of weights with \textit{learning\_rate=0.01} and 40000 iterations. The following combination yielded the highest accuracy of 91.43\% with a minimum accuracy of 77\% and an average accuracy of 82\%.
\begin{figure}[h]
\centering
\includegraphics[scale=1.0, width=5cm]{Fig21.png}
\caption{Plot of Loss vs Epochs for 40000 iterations with lr=1e-2}
\end{figure}
As seen in the figure below, a considerably high accuracy was observed even though the loss had significant fluctuations. This is due to the high learning rate provided. Lesser fluctuations were observed when the learning rate was decreased to 0.0005. But to compensate the training, the number of iterations had to increased significantly to around 1000000 to achieve an accuracy of 91.43\%
\begin{figure}[h]
\centering
\includegraphics[scale=1.0, width=5cm]{Fig22.png}
\caption{Plot of Loss vs Epochs for 40000 iterations with lr=5e-4}
\includegraphics[scale=1.0, width=5cm]{Fig23.png}
\caption{Plot of Loss vs Epochs for 1000000 iterations with lr=5e-4}
\end{figure}
\newline
\underline{Top metrics achieved by the model:}
\begin{itemize}
\item{Best testing accuracy: $91.43\%$}
\item{Best testing recall: $94.66\%$}
\item{Best testing precision: $89.30\%$}
\item{Best testing fscore: $91.90\%$}
\end{itemize}

\section*{\textcolor{blue}{6. Conclusion:}}
\begin{itemize}
\item{In general, a three layer network performs better than a two layer network. Adding more layers/more neurons to the hidden layers provides more trainable parameters which help the network to fit the model in a better way. But the drawback of adding multiple layers/mulitple neurons is that, the model requires more iterations to train. In this case, there is a higher chance that the model might overfit the data, and hence giving a low testing accuracy.}
\item{A lower learning rate requires more iterations to converge, but conversely, using a higher learning rate might lead to fluctuation in the loss/exploding gradient.}
\item{Different initialization of weights leads to convergence of the model at different local minima (initializations mentioned above).}
\item{Different activation functions for the hidden layers provides drastically varying results. We	 have used ReLU (Rectified Linear) which is by and the far, the de-facto standard activation used for hidden layers. Other activations like tanh or sigmoid gave very low accuracy as compared to that of ReLU. ReLU suffers from the \emph{dying neuron problem}, in which the output of a neuron might be 0 for most of the training (i.e. that neuron is not contributing to the network), and hence we tried using leaky-ReLU, but the accuracy was found to more or less the same.}
\end{itemize}
[results on next page]

\section*{\textcolor{blue}{7. Additional Work:}}
\subsection*{\textcolor{red}{7.1 Momentum and Decay}}
Additionally, we implemented Momentum and Decay to our SGD to accelerate the learning process. Momentum provides an \textit{exponentially weighted average} solution to the gradient according to the following formula: $V_t = \beta V_{t-1} + (1 - \beta)G$ where $G$ is the current gradient and $V_0 = 0$. Later the weights are updates according to: $W = W - \eta V_t$. The value $\beta$ is the momentum parameter (0 $\leq$ $\beta$ $\leq$ 1.0) and helps to retain the \textit{momentum} in the direction of the previous gradient while slowing \textit{forgetting} the older and much previous momentum directions. Decay of the learning rate is as the word suggests. The learning rate is reduced by a certain amount every few steps so that smaller and smaller steps will be taken as we approach the minimum and the chance of overshooting it will decrease.

\newpage
\onecolumn
\section*{\textcolor{blue}{8. Results:}}
\begin{figure}[h!]
\centering
\includegraphics[scale=1.0, width=5cm]{Fig1.png}
\includegraphics[scale=1.0, width=5cm]{Fig2.png}
\caption*{2-Layer NN using uniform distribution}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[scale=1.0, width=5cm]{Fig3.png}
\includegraphics[scale=1.0, width=5cm]{Fig4.png}
\caption*{3-Layer NN using uniform distirbution}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[scale=1.0, width=5cm]{Fig5.png}
\includegraphics[scale=1.0, width=5cm]{Fig6.png}
\caption*{2-Layer NN using normal distirbution}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[scale=1.0, width=5cm]{Fig7.png}
\includegraphics[scale=1.0, width=5cm]{Fig8.png}
\caption*{3-Layer NN using normal distirbution}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[scale=1.0, width=5cm]{Fig9.png}
\includegraphics[scale=1.0, width=5cm]{Fig10.png}
\caption*{3-Layer NN using uniform distirbution with momentum and decay}
\end{figure}
%=================================================================================================================%
\end{document}
